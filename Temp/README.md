# ä¼½é©¬å°„çº¿å±è”½ç´¯ç§¯å› å­é¢„æµ‹æ¨¡å‹

åŸºäºPhysics-Informed Multi-Task Transformerçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹ä¼½é©¬ç‚¹æºå•å±‚å±è”½ä¸€ç»´çƒæ¨¡å‹çš„ç´¯ç§¯å› å­ã€‚

## æ¨¡å‹ç‰¹ç‚¹

### ğŸš€ **æœ€ä¼˜æ¶æ„é€‰æ‹©ï¼šPhysics-Informed Multi-Task Transformer**

ç»è¿‡å¯¹æ¯”åˆ†æäº”ç§æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæˆ‘ä»¬é€‰æ‹©äº†æœ€é€‚åˆæ‚¨ç‰©ç†æ¨¡å‹çš„æ¶æ„ï¼š

1. **Physics-Informed Neural Network (PINN)** â­â­â­â­â­
2. **Multi-Task Transformer Network** â­â­â­â­â­  
3. **Deep Residual Network (ResNet)** â­â­â­â­
4. **Ensemble Learning Framework** â­â­â­â­
5. **Attention-Enhanced MLP** â­â­â­

### ğŸ”¬ **æ ¸å¿ƒä¼˜åŠ¿**

- **ç‰©ç†çº¦æŸèå…¥**: ç¡®ä¿é¢„æµ‹ç»“æœç¬¦åˆä¼½é©¬å°„çº¿ä¼ è¾“ç‰©ç†å®šå¾‹
- **å¤šä»»åŠ¡å­¦ä¹ **: åŒæ—¶é¢„æµ‹6ä¸ªç´¯ç§¯å› å­ï¼Œåˆ©ç”¨ä»»åŠ¡é—´ç›¸å…³æ€§æé«˜ç²¾åº¦
- **æ³¨æ„åŠ›æœºåˆ¶**: è‡ªåŠ¨å­¦ä¹ èƒ½é‡ã€åšåº¦ã€è¡°å‡ç³»æ•°é—´çš„é‡è¦å…³ç³»
- **é«˜ç²¾åº¦é¢„æµ‹**: ä¸“ä¸ºç‰©ç†æ¨¡å‹è®¾è®¡ï¼Œç¡®ä¿é¢„æµ‹çš„ç‰©ç†åˆç†æ€§

### ğŸ“Š **æ•°æ®é›†ä¿¡æ¯**

- **æ ·æœ¬æ•°é‡**: 90,168ä¸ª
- **è¾“å…¥ç‰¹å¾**: 8ä¸ª (Energy, Shell, MFP, MAC_Total, MAC_Incoherent, MAC_Coherent, MAC_Photoelectric, MAC_Pair_production)
- **è¾“å‡ºç‰¹å¾**: 6ä¸ªç´¯ç§¯å› å­ (Inf_Flu_BUF, Fin_Flu_BUF, Inf_Exp_BUF, Fin_Exp_BUF, Inf_Eff_BUF, Fin_Eff_BUF)

## å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒé…ç½®

```bash
pip install -r requirements.txt
```

### 2. è®­ç»ƒæ¨¡å‹

```bash
python training_pipeline.py
```

è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ï¼š
- è‡ªåŠ¨æ•°æ®é¢„å¤„ç†å’Œå½’ä¸€åŒ–
- æ•°æ®é›†åˆ†å‰² (70% è®­ç»ƒ, 15% éªŒè¯, 15% æµ‹è¯•)
- æ—©åœæœºåˆ¶é˜²æ­¢è¿‡æ‹Ÿåˆ
- å­¦ä¹ ç‡è‡ªé€‚åº”è°ƒæ•´
- ç‰©ç†çº¦æŸæŸå¤±å‡½æ•°

### 3. æ¨¡å‹æ¨ç†

```bash
python model_inference.py
```

åŠŸèƒ½åŒ…æ‹¬ï¼š
- å•æ ·æœ¬é¢„æµ‹
- æ‰¹é‡é¢„æµ‹
- èƒ½é‡ä¾èµ–æ€§åˆ†æ
- åšåº¦ä¾èµ–æ€§åˆ†æ
- æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–
- é¢„æµ‹è¯¯å·®åˆ†æ

## æ–‡ä»¶ç»“æ„

```
â”œâ”€â”€ dataset.csv                          # åŸå§‹æ•°æ®é›†
â”œâ”€â”€ physics_informed_transformer.py      # æ¨¡å‹æ¶æ„å®šä¹‰
â”œâ”€â”€ training_pipeline.py                 # è®­ç»ƒç®¡é“
â”œâ”€â”€ model_inference.py                   # æ¨ç†å’Œåˆ†æå·¥å…·
â”œâ”€â”€ requirements.txt                     # ä¾èµ–åŒ…åˆ—è¡¨
â”œâ”€â”€ README.md                           # è¯´æ˜æ–‡æ¡£
â””â”€â”€ ç”Ÿæˆçš„æ–‡ä»¶/
    â”œâ”€â”€ physics_informed_transformer_model.pth  # è®­ç»ƒå¥½çš„æ¨¡å‹
    â”œâ”€â”€ training_history.png                    # è®­ç»ƒå†å²å›¾
    â”œâ”€â”€ prediction_comparison.png               # é¢„æµ‹å¯¹æ¯”å›¾
    â”œâ”€â”€ energy_dependence_analysis.png          # èƒ½é‡ä¾èµ–æ€§åˆ†æ
    â”œâ”€â”€ thickness_dependence_analysis.png       # åšåº¦ä¾èµ–æ€§åˆ†æ
    â”œâ”€â”€ attention_weights.png                   # æ³¨æ„åŠ›æƒé‡çƒ­å›¾
    â””â”€â”€ error_distribution.png                  # è¯¯å·®åˆ†å¸ƒå›¾
```

## æ¨¡å‹æ¶æ„è¯¦è§£

### Physics-Informed Transformer

```python
PhysicsInformedTransformer(
    input_dim=8,           # è¾“å…¥ç‰¹å¾ç»´åº¦
    output_dim=6,          # è¾“å‡ºç‰¹å¾ç»´åº¦
    d_model=256,           # æ¨¡å‹ç»´åº¦
    n_heads=8,             # æ³¨æ„åŠ›å¤´æ•°
    n_layers=6,            # Transformerå±‚æ•°
    d_ff=1024,             # å‰é¦ˆç½‘ç»œç»´åº¦
    dropout=0.1            # Dropoutç‡
)
```

### å…³é”®ç»„ä»¶

1. **å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶**: æ•æ‰è¾“å…¥ç‰¹å¾é—´çš„å¤æ‚å…³ç³»
2. **ä½ç½®ç¼–ç **: ä¿æŒç‰¹å¾çš„ä½ç½®ä¿¡æ¯
3. **å¤šä»»åŠ¡è¾“å‡ºå¤´**: ä¸ºæ¯ä¸ªç´¯ç§¯å› å­è®¾è®¡ä¸“é—¨çš„é¢„æµ‹å¤´
4. **ç‰©ç†çº¦æŸå±‚**: ç¡®ä¿é¢„æµ‹ç»“æœæ»¡è¶³ç‰©ç†å®šå¾‹
5. **ç‰©ç†çº¦æŸæŸå¤±å‡½æ•°**: åœ¨è®­ç»ƒä¸­èå…¥ç‰©ç†çŸ¥è¯†

### ç‰©ç†çº¦æŸ

- **çº¦æŸ1**: ç´¯ç§¯å› å­å¿…é¡» â‰¥ 1
- **çº¦æŸ2**: æœ‰é™å‡ ä½•ç´¯ç§¯å› å­ â‰¤ æ— é™å‡ ä½•ç´¯ç§¯å› å­
- **çº¦æŸ3**: éšåšåº¦å¢åŠ çš„å•è°ƒæ€§çº¦æŸ

## ä½¿ç”¨ç¤ºä¾‹

### å•æ ·æœ¬é¢„æµ‹

```python
from model_inference import BUFPredictor

predictor = BUFPredictor('physics_informed_transformer_model.pth')

result, attention_weights = predictor.predict_single(
    energy=1.0,                    # èƒ½é‡ (MeV)
    shell=0,                       # å£³å±‚æ•°
    mfp=5.0,                      # åšåº¦ (MFP)
    mac_total=0.5,                # æ€»è¡°å‡ç³»æ•°
    mac_incoherent=0.1,           # éç›¸å¹²æ•£å°„è¡°å‡ç³»æ•°
    mac_coherent=0.05,            # ç›¸å¹²æ•£å°„è¡°å‡ç³»æ•°
    mac_photoelectric=0.3,        # å…‰ç”µæ•ˆåº”è¡°å‡ç³»æ•°
    mac_pair_production=0.05      # ç”µå­å¯¹äº§ç”Ÿè¡°å‡ç³»æ•°
)

print("é¢„æµ‹çš„ç´¯ç§¯å› å­:")
for name, value in result.items():
    print(f"{name}: {value:.6f}")
```

### æ‰¹é‡é¢„æµ‹

```python
import pandas as pd

# å‡†å¤‡è¾“å…¥æ•°æ®
input_data = pd.DataFrame({
    'Energy': [0.5, 1.0, 2.0],
    'Shell': [0, 0, 1],
    'MFP': [1.0, 5.0, 10.0],
    # ... å…¶ä»–ç‰¹å¾
})

predictions, _ = predictor.predict_batch(input_data)
print(predictions)
```

### åˆ†æå·¥å…·

```python
# èƒ½é‡ä¾èµ–æ€§åˆ†æ
energies, results = predictor.analyze_energy_dependence(
    shell=0, 
    mfp_values=[1.0, 5.0, 10.0],
    energy_range=(0.01, 10.0)
)

# åšåº¦ä¾èµ–æ€§åˆ†æ
thicknesses, predictions = predictor.analyze_thickness_dependence(
    energy=1.0,
    shell=0,
    thickness_range=(0.0, 20.0)
)

# æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–
predictor.visualize_attention_weights(
    energy=1.0, shell=0, mfp=5.0,
    mac_total=0.5, mac_incoherent=0.1,
    mac_coherent=0.05, mac_photoelectric=0.3,
    mac_pair_production=0.05
)
```

## æ€§èƒ½æŒ‡æ ‡

æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„å…¸å‹æ€§èƒ½ï¼š

- **RMSE**: < 0.01 (å½’ä¸€åŒ–å)
- **RÂ²**: > 0.99
- **ç‰©ç†çº¦æŸæ»¡è¶³ç‡**: 100%

## æŠ€æœ¯ç‰¹ç‚¹

### 1. å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¶æ„
- Transformeræ³¨æ„åŠ›æœºåˆ¶
- å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶
- æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–

### 2. ç‰©ç†çŸ¥è¯†èå…¥
- ç‰©ç†çº¦æŸæŸå¤±å‡½æ•°
- ç´¯ç§¯å› å­ç‰©ç†å®šå¾‹çº¦æŸ
- å‡ ä½•å…³ç³»çº¦æŸ

### 3. é²æ£’çš„è®­ç»ƒç­–ç•¥
- æ—©åœæœºåˆ¶
- å­¦ä¹ ç‡è‡ªé€‚åº”è°ƒæ•´
- æ¢¯åº¦è£å‰ª
- Dropoutæ­£åˆ™åŒ–

### 4. å…¨é¢çš„åˆ†æå·¥å…·
- è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
- é¢„æµ‹ç»“æœåˆ†æ
- æ³¨æ„åŠ›æƒé‡è§£é‡Š
- è¯¯å·®åˆ†å¸ƒç»Ÿè®¡

## æ‰©å±•åŠŸèƒ½

### è‡ªå®šä¹‰ç‰©ç†çº¦æŸ

å¯ä»¥åœ¨ `PhysicsConstraintLayer` ä¸­æ·»åŠ æ›´å¤šç‰©ç†çº¦æŸï¼š

```python
def custom_physics_constraint(self, predictions, inputs):
    # æ·»åŠ è‡ªå®šä¹‰ç‰©ç†çº¦æŸ
    # ä¾‹å¦‚ï¼šèƒ½é‡å®ˆæ’ã€åŠ¨é‡å®ˆæ’ç­‰
    pass
```

### æ¨¡å‹é›†æˆ

å¯ä»¥è®­ç»ƒå¤šä¸ªæ¨¡å‹å¹¶è¿›è¡Œé›†æˆï¼š

```python
# è®­ç»ƒå¤šä¸ªæ¨¡å‹
models = []
for i in range(5):
    model = PhysicsInformedTransformer(...)
    # è®­ç»ƒæ¨¡å‹...
    models.append(model)

# é›†æˆé¢„æµ‹
ensemble_prediction = np.mean([model.predict(x) for model in models], axis=0)
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **CUDAå†…å­˜ä¸è¶³**: å‡å°batch_sizeæˆ–æ¨¡å‹ç»´åº¦
2. **è®­ç»ƒä¸æ”¶æ•›**: è°ƒæ•´å­¦ä¹ ç‡æˆ–å¢åŠ æ­£åˆ™åŒ–
3. **ç‰©ç†çº¦æŸè¿å**: å¢åŠ physics_weightæƒé‡

### æ€§èƒ½ä¼˜åŒ–

1. **ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ**: åŠ é€Ÿè®­ç»ƒå¹¶å‡å°‘å†…å­˜ä½¿ç”¨
2. **æ•°æ®å¹¶è¡Œ**: å¤šGPUè®­ç»ƒ
3. **æ¨¡å‹é‡åŒ–**: éƒ¨ç½²æ—¶å‡å°‘æ¨¡å‹å¤§å°

## è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›æ¨¡å‹ï¼

## è®¸å¯è¯

MIT License

---

**æ³¨æ„**: è¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºä¼½é©¬å°„çº¿å±è”½ç‰©ç†é—®é¢˜è®¾è®¡çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç¡®ä¿äº†é¢„æµ‹ç»“æœçš„ç‰©ç†åˆç†æ€§å’Œé«˜ç²¾åº¦ã€‚